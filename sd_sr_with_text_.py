# -*- coding: utf-8 -*-
"""sd_sr_with_text"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQ2CPbPicC-4LQWna_JiISQvQzwBZrJj

# Implementation of SD for super-resolution on advertising images (with text)
"""

# Install required packages
!pip install opencv-python matplotlib numpy torch torchvision diffusers transformers accelerate xformers
!pip install git+https://github.com/openai/CLIP.git
!pip install pytesseract

"""## Import Libraries"""

import os
import cv2
import numpy as np
import torch
from diffusers import StableDiffusionImg2ImgPipeline, UniPCMultistepScheduler
import matplotlib.pyplot as plt
from google.colab import drive
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
from scipy.ndimage import gaussian_filter
import clip
import pytesseract
from PIL import Image
from huggingface_hub import login
import gc

"""## Mount Google Drive and Setup"""

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Authenticate with Hugging Face
from google.colab import userdata

try:
    hf_token = userdata.get('HF_TOKEN')
    if hf_token:
        os.environ['HUGGINGFACE_TOKEN'] = hf_token
        login(token=hf_token)
        print("✓ Successfully authenticated with Hugging Face!")
    else:
        print("⚠️ HF_TOKEN not found in secrets")
except Exception as e:
    print(f"❌ Authentication error: {e}")

# Set up directories
drive_input_dir = '/content/drive/MyDrive/images_with_text'
drive_output_dir = '/content/drive/MyDrive/stablesr_final_output_with_text'
drive_degraded_dir = '/content/drive/MyDrive/gan_degraded_images'

local_input_dir = '/content/input_images'
local_output_dir = '/content/output_images'
local_degraded_dir = '/content/degraded_images'

# Create directories
for dir_path in [drive_output_dir, drive_degraded_dir, local_input_dir, local_output_dir, local_degraded_dir]:
    os.makedirs(dir_path, exist_ok=True)

"""## Memory management"""

def clear_gpu_memory():
    """Clear GPU memory"""
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

def get_max_image_size():
    """Get maximum image size based on available GPU memory"""
    # Get available GPU memory in GB
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)

    # Conservative estimate: max dimension based on GPU memory
    if gpu_memory > 32:
        return 2048
    elif gpu_memory > 16:
        return 1536
    elif gpu_memory > 8:
        return 1024
    else:
        return 768

"""## Copy Images from Google Drive"""

def copy_images_from_drive(drive_input_dir, local_input_dir):
    """Copy images from Google Drive to local directory"""
    if not os.path.exists(drive_input_dir):
        print(f"Error: Directory {drive_input_dir} does not exist.")
        return []

    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
    image_files = [f for f in os.listdir(drive_input_dir)
                  if os.path.isfile(os.path.join(drive_input_dir, f)) and
                  f.lower().endswith(image_extensions)]

    if not image_files:
        print(f"No image files found in {drive_input_dir}")
        return []

    print(f"Found {len(image_files)} images. Copying...")
    for filename in image_files:
        src_path = os.path.join(drive_input_dir, filename)
        dst_path = os.path.join(local_input_dir, filename)
        os.system(f'cp "{src_path}" "{dst_path}"')
    return image_files

"""## Create Degraded images"""

def create_degraded_images(image_files, local_input_dir, local_degraded_dir, drive_degraded_dir, scale_factor=0.5):
    """Create degraded versions with less aggressive downscaling"""
    degraded_files = []
    for filename in image_files:
        input_path = os.path.join(local_input_dir, filename)
        degraded_filename = os.path.splitext(filename)[0] + '_degraded' + os.path.splitext(filename)[1]
        degraded_path = os.path.join(local_degraded_dir, degraded_filename)

        img = cv2.imread(input_path, cv2.IMREAD_COLOR)
        if img is None:
            continue

        h, w = img.shape[:2]
        lr_h, lr_w = int(h * scale_factor), int(w * scale_factor)

        # Downscale and degrade
        img_lr = cv2.resize(img, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)
        _, buffer = cv2.imencode('.jpg', img_lr, [cv2.IMWRITE_JPEG_QUALITY, 50])
        img_lr = cv2.imdecode(buffer, cv2.IMREAD_COLOR)
        img_lr = cv2.GaussianBlur(img_lr, (3, 3), 0.8)

        cv2.imwrite(degraded_path, img_lr)
        os.system(f'cp "{degraded_path}" "{os.path.join(drive_degraded_dir, degraded_filename)}"')
        degraded_files.append(degraded_filename)
    return degraded_files

"""## Initialize CLIP Model for Evaluation"""

def load_clip_model():
    """Load CLIP model"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

"""## OCR"""

def calculate_ocr_accuracy(original_img_path, enhanced_img_path):
    """Calculate OCR accuracy for text in images"""
    try:
        def preprocess_for_ocr(img_path):
            img = cv2.imread(img_path)
            if img is None:
                return None
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            kernel = np.ones((1, 1), np.uint8)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
            return thresh

        orig_preprocessed = preprocess_for_ocr(original_img_path)
        enhanced_preprocessed = preprocess_for_ocr(enhanced_img_path)

        if orig_preprocessed is None or enhanced_preprocessed is None:
            return 0.0

        # Try multiple OCR configurations
        config = r'--oem 3 --psm 6'  # Assume a single uniform block of text
        original_text = pytesseract.image_to_string(orig_preprocessed, config=config)
        enhanced_text = pytesseract.image_to_string(enhanced_preprocessed, config=config)

        if not original_text.strip():
            config = r'--oem 3 --psm 11'  # Sparse text
            original_text = pytesseract.image_to_string(orig_preprocessed, config=config)
            enhanced_text = pytesseract.image_to_string(enhanced_preprocessed, config=config)

        if not original_text.strip():
            config = r'--oem 3 --psm 12'  # Dense text with OSD
            original_text = pytesseract.image_to_string(original_img_path, config=config)
            enhanced_text = pytesseract.image_to_string(enhanced_img_path, config=config)

        if not original_text.strip():
            print(f"Warning: No text detected in images")
            return 0.0

        # Use Jaccard similarity for comparison
        def get_words(text):
            return set(text.lower().strip().split())

        original_words = get_words(original_text)
        enhanced_words = get_words(enhanced_text)

        if len(original_words) == 0:
            return 0.0

        common_words = original_words.intersection(enhanced_words)
        all_words = original_words.union(enhanced_words)

        # Print detected words for debugging
        print(f"Original text words: {original_words}")
        print(f"Enhanced text words: {enhanced_words}")
        print(f"Common words: {common_words}")

        if len(all_words) == 0:
            return 0.0

        accuracy = len(common_words) / len(all_words) * 100
        return accuracy

    except Exception as e:
        print(f"OCR error: {e}")
        return 0.0

"""## Metrics Functions"""

def calculate_metrics(original_img, enhanced_img, clip_model=None, clip_preprocess=None, prompt=None, device=None):
    """Calculate image quality metrics"""
    if original_img.shape != enhanced_img.shape:
        enhanced_img = cv2.resize(enhanced_img, (original_img.shape[1], original_img.shape[0]),
                                 interpolation=cv2.INTER_CUBIC)

    def rgb_to_y(img):
        return 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]

    original_y = rgb_to_y(original_img)
    enhanced_y = rgb_to_y(enhanced_img)

    psnr = peak_signal_noise_ratio(original_y, enhanced_y, data_range=255)
    ssim = structural_similarity(original_y, enhanced_y, data_range=255)

    def calculate_sharpness(img):
        gray = img if len(img.shape) == 2 else rgb_to_y(img)
        return cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F).var()

    original_sharpness = calculate_sharpness(original_img)
    enhanced_sharpness = calculate_sharpness(enhanced_img)
    sharpness_increase = (enhanced_sharpness - original_sharpness) / (original_sharpness + 1e-8) * 100

    def calculate_details(img):
        gray = img if len(img.shape) == 2 else rgb_to_y(img)
        blurred = gaussian_filter(gray, sigma=3)
        high_freq = np.abs(gray - blurred)
        return np.mean(high_freq)

    original_details = calculate_details(original_img)
    enhanced_details = calculate_details(enhanced_img)
    detail_increase = (enhanced_details - original_details) / (original_details + 1e-8) * 100

    metrics = {
        'psnr': psnr,
        'ssim': ssim,
        'sharpness_increase': sharpness_increase,
        'detail_increase': detail_increase
    }

    # Add CLIP metrics if available
    if clip_model is not None and clip_preprocess is not None and device is not None:
        try:
            original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
            enhanced_rgb = cv2.cvtColor(enhanced_img, cv2.COLOR_BGR2RGB)

            original_pil = Image.fromarray(original_rgb)
            enhanced_pil = Image.fromarray(enhanced_rgb)

            with torch.no_grad():
                original_features = clip_model.encode_image(clip_preprocess(original_pil).unsqueeze(0).to(device))
                enhanced_features = clip_model.encode_image(clip_preprocess(enhanced_pil).unsqueeze(0).to(device))

                original_features = original_features / original_features.norm(dim=-1, keepdim=True)
                enhanced_features = enhanced_features / enhanced_features.norm(dim=-1, keepdim=True)

                clip_img_similarity = torch.nn.functional.cosine_similarity(original_features, enhanced_features).item()
                metrics['clip_img_similarity'] = clip_img_similarity

                if prompt:
                    text = clip.tokenize([prompt]).to(device)
                    text_features = clip_model.encode_text(text)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    prompt_img_similarity = torch.nn.functional.cosine_similarity(enhanced_features, text_features).item()
                    metrics['prompt_img_similarity'] = prompt_img_similarity
        except Exception as e:
            print(f"CLIP metric error: {e}")

    return metrics

"""## Define StableSR Model"""

def load_stablesr_model():
    """Load simplified StableSR model - only img2img for better memory efficiency"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    try:
        # Load only img2img pipeline for memory efficiency
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16,  # Use float16 for memory savings
            use_auth_token=True,
            safety_checker=None,
            requires_safety_checker=False,
            variant="fp16",  # Load FP16 variant
        )
        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

        # Enable memory efficient attention
        pipe.enable_model_cpu_offload()
        pipe.enable_vae_slicing()  # Process VAE in slices
        pipe.enable_vae_tiling()   # Process VAE with tiling
        if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
            pipe.enable_xformers_memory_efficient_attention()

        # Clear memory after loading
        clear_gpu_memory()

        print("Successfully loaded StableSR model")
        return pipe, device
    except Exception as e:
        print(f"Error loading model: {e}")
        return None, device

def get_text_prompts():
    """Get ultra-focused prompts for maximum text clarity"""
    return [
        "Three vibrant soda cans (“Poppi” strawberry lemonade, orange, lemon-lime) with fresh water droplets on a bright pink-orange gradient background, photorealistic packaging render, ultra-detailed, super-resolution, crisp reflections and highlights on metal",
        "A sleek red Coca-Cola Zero Sugar can soaring through a clear blue sky, leaving a swirling cloud-trail spelling “Coca-Cola”, photorealistic product shot, ultra-detailed, super-resolution, soft cloud textures and metallic can highlights",
        "Retro poster of an ecstatic cartoon man holding a foaming beer mug, bold typography “BrewCo Draught House” and “Extreme beers are extremely satisfying”, stylized vintage halftone illustration, ultra-detailed, super-resolution, textured paper grain",
        "Classic poster of a torn-paper reveal showing a Campari bottle on a black background, elegant script “Bitter Campari l’aperitivo”, retro print style, ultra-detailed, super-resolution, subtle paper texture and distressed edges",
        "Vintage French ad poster: overhead view of a waiter’s tray with a Berger 45 bottle and two glasses on a bright yellow field, bold black “Berger 45 Type Marseillais” text, stylized flat colors, ultra-detailed, super-resolution, textured poster look",
        "Clean 300 ml glass juice bottle labeled “Tasty Drink” on a white background, fresh orange slice and whole orange behind, Healthy & Tasty logo and Arabic slogan, photorealistic packaging render, ultra-detailed, super-resolution, crisp glass reflections",
        "Sunset coastal road scene with red convertible parked at edge, warm orange sky and setting sun over the ocean, bold retro text “Experience the Open Road” and “Freedom in Every Drive”, stylized illustration, ultra-detailed, 8k super-resolution, film-grain texture",
        "Vintage-style ad of a glossy green muscle car on textured sand-colored paper, bold heading “A New Kind of Excitement!”, minimalist layout, ultra-detailed, super-resolution, subtle halftone print effect",
        "Retro automotive poster: red convertible Speedster 1972 on flat ochre background, bold black “Freedom on Wheels” and “0–60 in 7 seconds” text, stylized illustration, ultra-detailed, super-resolution, textured paper finish",
        "Classic car ad on aged beige paper: blue Falcon GT sedan with soft shadow, bold headline “Meet the Legend: Falcon GT – Power and Style in One”, retro print style, ultra-detailed, super-resolution, vintage grain texture"
    ]

"""## Process Images"""

def process_image_stablesr(degraded_path, output_path, pipe, device, prompt,
                          clip_model=None, clip_preprocess=None, original_path=None,
                          num_inference_steps=15):
    """Process a single image with simplified memory-efficient approach"""
    img = cv2.imread(degraded_path, cv2.IMREAD_COLOR)
    if img is None:
        print(f"Error reading {degraded_path}")
        return None, None, None

    print(f"Processing: {degraded_path}")

    # Convert to PIL and resize to manageable size
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_pil = Image.fromarray(img_rgb)

    # Get max dimensions based on GPU memory
    max_size = get_max_image_size()

    # Scale to manageable size for processing
    scale_factor = 3  # Reduced from 4 to save memory
    target_width = min(img_pil.width * scale_factor, max_size)
    target_height = min(img_pil.height * scale_factor, max_size)

    # Maintain aspect ratio
    aspect_ratio = img_pil.width / img_pil.height
    if target_width / target_height > aspect_ratio:
        target_width = int(target_height * aspect_ratio)
    else:
        target_height = int(target_width / aspect_ratio)

    img_upscaled = img_pil.resize((target_width, target_height), Image.BICUBIC)

    try:
        # Clear memory before processing
        clear_gpu_memory()

        # Process with very conservative settings
        with torch.no_grad():
            result = pipe(
                prompt=prompt,
                image=img_upscaled,
                strength=0.25,  # Very low strength for text preservation
                guidance_scale=3.5,  # Low guidance to avoid changing content
                num_inference_steps=num_inference_steps,
            ).images[0]

        # Clear memory after processing
        clear_gpu_memory()

        # Convert back to OpenCV format
        enhanced_img = cv2.cvtColor(np.array(result), cv2.COLOR_RGB2BGR)

        # Additional post-processing for text enhancement
        kernel = np.array([[0, -0.5, 0], [-0.5, 3, -0.5], [0, -0.5, 0]])
        enhanced_img = cv2.filter2D(enhanced_img, -1, kernel)

        # Save result
        cv2.imwrite(output_path, enhanced_img)
        print(f"Saved: {output_path}")

        # Calculate metrics
        metrics = calculate_metrics(
            img, enhanced_img,
            clip_model=clip_model,
            clip_preprocess=clip_preprocess,
            prompt=prompt,
            device=device
        )

        # Calculate OCR accuracy
        if original_path:
            ocr_accuracy = calculate_ocr_accuracy(original_path, output_path)
            metrics['ocr_accuracy'] = ocr_accuracy
            print(f"OCR Accuracy: {ocr_accuracy:.2f}%")

        # Print metrics
        print(f"PSNR: {metrics['psnr']:.2f} dB")
        print(f"SSIM: {metrics['ssim']:.4f}")
        print(f"Sharpness increase: {metrics['sharpness_increase']:.2f}%")

        return img, enhanced_img, metrics

    except Exception as e:
        print(f"Processing error: {e}")
        clear_gpu_memory()
        import traceback
        traceback.print_exc()
        return None, None, None

def process_all_images_stablesr(image_files, pipe, device, clip_model, clip_preprocess,
                               local_input_dir, local_degraded_dir, local_output_dir, drive_output_dir,
                               prompts=None):
    """Process all images with StableSR"""
    all_metrics = []

    # Make sure we have enough prompts
    if prompts and len(prompts) < len(image_files):
        prompts = (prompts * ((len(image_files) // len(prompts)) + 1))[:len(image_files)]

    for i, filename in enumerate(image_files):
        # Clear memory before each image
        clear_gpu_memory()

        base_filename = os.path.splitext(filename)[0]
        degraded_filename = base_filename + '_degraded' + os.path.splitext(filename)[1]

        input_path = os.path.join(local_input_dir, filename)
        degraded_path = os.path.join(local_degraded_dir, degraded_filename)
        output_filename = base_filename + '_StableSR.png'
        output_path = os.path.join(local_output_dir, output_filename)

        # Get prompt for this image
        prompt = prompts[i] if prompts and i < len(prompts) else "high quality product with clear readable text"

        print(f"\n{'='*80}\nProcessing {filename} with StableSR...")
        print(f"Prompt: {prompt}")

        try:
            degraded_img, output_img, metrics = process_image_stablesr(
                degraded_path, output_path, pipe, device, prompt,
                clip_model=clip_model, clip_preprocess=clip_preprocess, original_path=input_path
            )

            if degraded_img is None or output_img is None:
                print(f"Error processing {filename}")
                continue

            # Store metrics
            metrics['filename'] = filename
            metrics['model'] = "StableSR"
            all_metrics.append(metrics)

            # Copy to Google Drive
            drive_output_path = os.path.join(drive_output_dir, output_filename)
            os.system(f'cp "{output_path}" "{drive_output_path}"')
            print(f"Saved to Drive: {drive_output_path}")

            # Display comparison
            orig_img = cv2.imread(input_path, cv2.IMREAD_COLOR)
            orig_img_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
            degraded_img_rgb = cv2.cvtColor(degraded_img, cv2.COLOR_BGR2RGB)
            output_img_rgb = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)

            plt.figure(figsize=(20, 12))

            # Original
            plt.subplot(1, 3, 1)
            plt.imshow(orig_img_rgb)
            plt.title(f"Original: {filename}\nDimensions: {orig_img.shape[1]}x{orig_img.shape[0]}", fontsize=12)
            plt.axis('off')

            # Degraded
            plt.subplot(1, 3, 2)
            degraded_resized = cv2.resize(degraded_img_rgb, (orig_img.shape[1], orig_img.shape[0]), interpolation=cv2.INTER_NEAREST)
            plt.imshow(degraded_resized)
            plt.title(f"Degraded: {degraded_filename}\nDimensions: {degraded_img.shape[1]}x{degraded_img.shape[0]}\n(Resized for display)", fontsize=12)
            plt.axis('off')

            # Enhanced
            plt.subplot(1, 3, 3)
            enhanced_resized = cv2.resize(output_img_rgb, (orig_img.shape[1], orig_img.shape[0]), interpolation=cv2.INTER_CUBIC) if output_img.shape != orig_img.shape else output_img_rgb
            plt.imshow(enhanced_resized)

            title_text = f"Enhanced (StableSR): {output_filename}\nDimensions: {output_img.shape[1]}x{output_img.shape[0]}\n"
            title_text += f"PSNR: {metrics['psnr']:.2f} dB, SSIM: {metrics['ssim']:.4f}\n"
            title_text += f"Sharpness Increase: {metrics['sharpness_increase']:.2f}%"

            if 'ocr_accuracy' in metrics:
                title_text += f"\nOCR Accuracy: {metrics['ocr_accuracy']:.2f}%"
            if 'clip_img_similarity' in metrics:
                title_text += f"\nCLIP Image Sim: {metrics['clip_img_similarity']:.4f}"
            if 'prompt_img_similarity' in metrics:
                title_text += f", Prompt Sim: {metrics['prompt_img_similarity']:.4f}"

            plt.title(title_text, fontsize=12)
            plt.axis('off')

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Error processing {filename}: {e}")
            import traceback
            traceback.print_exc()

    print(f"\nProcessing complete! Processed {len(all_metrics)} images")
    return all_metrics

"""## Compare Results Function"""

def compare_model_results(all_results, model1="StableSR"):
    """Analyze results from StableSR including OCR metric"""
    if not all_results:
        print("No results to analyze")
        return

    metrics_to_average = ['psnr', 'ssim', 'sharpness_increase', 'detail_increase', 'clip_img_similarity', 'prompt_img_similarity', 'ocr_accuracy']
    avg_metrics = {}

    for metric in metrics_to_average:
        valid_results = [r[metric] for r in all_results if metric in r]
        if valid_results:
            avg_metrics[metric] = sum(valid_results) / len(valid_results)

    print(f"\nAverage Metrics for {model1}:")
    print(f"{'Metric':<20} {'Value':<15}")
    print("-" * 35)
    for metric, value in avg_metrics.items():
        print(f"{metric:<20} {value:<15.4f}")

    # Create visualization including OCR
    metrics_to_plot = ['psnr', 'ssim', 'clip_img_similarity', 'ocr_accuracy']
    labels = ['PSNR (dB)', 'SSIM', 'CLIP Image Similarity', 'OCR Accuracy (%)']

    values = []
    for metric in metrics_to_plot:
        if metric in avg_metrics:
            if metric == 'ssim' or metric == 'clip_img_similarity':
                values.append(avg_metrics[metric] * 100)
            else:
                values.append(avg_metrics[metric])
        else:
            values.append(0)

    plt.figure(figsize=(12, 6))
    bars = plt.bar(labels, values, color=['skyblue', 'lightgreen', 'coral', 'gold'])

    for bar, value in zip(bars, values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{value:.2f}', ha='center', va='bottom')

    plt.ylabel('Values')
    plt.title(f'Average Image Quality Metrics - {model1}')
    plt.ylim(0, max(values) * 1.2)

    plt.tight_layout()
    plt.show()

"""## Main Part"""

def main():
    # Copy images from Drive
    image_files = copy_images_from_drive(drive_input_dir, local_input_dir)

    if not image_files:
        print("No images found!")
        return

    # Create degraded versions with less aggressive downscaling
    print("\n=== Creating degraded images ===")
    create_degraded_images(image_files, local_input_dir, local_degraded_dir, drive_degraded_dir, scale_factor=0.5)

    # Load CLIP model
    print("\n=== Loading CLIP model ===")
    clip_model, clip_preprocess, clip_device = load_clip_model()

    # Load StableSR
    print("\n=== Loading StableSR model ===")
    pipe, device = load_stablesr_model()

    if pipe is None:
        print("Failed to load model. Exiting.")
        return

    # Get prompts
    prompts = get_text_prompts()

    # Process images
    print("\n=== Processing with StableSR ===")
    stablesr_results = process_all_images_stablesr(
        image_files=image_files,
        pipe=pipe,
        device=device,
        clip_model=clip_model,
        clip_preprocess=clip_preprocess,
        local_input_dir=local_input_dir,
        local_degraded_dir=local_degraded_dir,
        local_output_dir=local_output_dir,
        drive_output_dir=drive_output_dir,
        prompts=prompts
    )

    # Analyze results
    print("\n=== Analyzing Results ===")
    compare_model_results(stablesr_results, "StableSR")

    print(f"\nResults saved to: {drive_output_dir}")

if __name__ == "__main__":
    main()

