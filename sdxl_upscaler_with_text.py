# -*- coding: utf-8 -*-
"""sdxl_upscaler_with_text

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AiCGqQYqwEC8LhjNkxvnm_g6s1CP3qPg

# Implementation of SDXL x4 Upscaler for super-resolution on product images (with text)
"""

# Install required packages
!pip install opencv-python matplotlib numpy torch torchvision diffusers transformers accelerate xformers
!pip install git+https://github.com/openai/CLIP.git
!pip install pytesseract

"""## Import Libraries"""

import os
import cv2
import numpy as np
import torch
from diffusers import StableDiffusionUpscalePipeline, DDIMScheduler
import matplotlib.pyplot as plt
from google.colab import drive
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
from scipy.ndimage import gaussian_filter
import clip
import pytesseract
from PIL import Image
from huggingface_hub import login
import gc
import time

"""## Mount Google Drive and Setup"""

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Authenticate with Hugging Face
from google.colab import userdata

try:
    hf_token = userdata.get('HF_TOKEN')
    if hf_token:
        os.environ['HUGGINGFACE_TOKEN'] = hf_token
        login(token=hf_token)
        print("✓ Successfully authenticated with Hugging Face!")
    else:
        print("⚠️ HF_TOKEN not found in secrets")
except Exception as e:
    print(f"❌ Authentication error: {e}")

# Set up directories
drive_input_dir = '/content/drive/MyDrive/images_with_text'
drive_output_dir = '/content/drive/MyDrive/sdxl_output_with_text'
drive_degraded_dir = '/content/drive/MyDrive/gan_degraded_images'

local_input_dir = '/content/input_images'
local_output_dir = '/content/output_images'
local_degraded_dir = '/content/degraded_images'

# Create directories
for dir_path in [drive_output_dir, drive_degraded_dir, local_input_dir, local_output_dir, local_degraded_dir]:
    os.makedirs(dir_path, exist_ok=True)

"""## Memory management functions"""

def clear_gpu_memory():
    """Clear GPU memory"""
    gc.collect()
    torch.cuda.empty_cache()
    if hasattr(torch.cuda, 'ipc_collect'):
        torch.cuda.ipc_collect()

def get_max_image_size():
    """Get maximum image size based on available GPU memory"""
    if not torch.cuda.is_available():
        return 1024  # Default for CPU

    # Get available GPU memory in GB
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)

    # Conservative estimate
    if gpu_memory > 32:
        return 2048
    elif gpu_memory > 16:
        return 1536
    elif gpu_memory > 8:
        return 1024
    else:
        return 768

"""## Copy Images from Google Drive"""

def copy_images_from_drive(drive_input_dir, local_input_dir):
    """Copy images from Google Drive to local directory"""
    if not os.path.exists(drive_input_dir):
        print(f"Error: Directory {drive_input_dir} does not exist.")
        return []

    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
    image_files = [f for f in os.listdir(drive_input_dir)
                  if os.path.isfile(os.path.join(drive_input_dir, f)) and
                  f.lower().endswith(image_extensions)]

    if not image_files:
        print(f"No image files found in {drive_input_dir}")
        return []

    print(f"Found {len(image_files)} images. Copying...")
    for filename in image_files:
        src_path = os.path.join(drive_input_dir, filename)
        dst_path = os.path.join(local_input_dir, filename)
        os.system(f'cp "{src_path}" "{dst_path}"')
    return image_files

"""## Create Degraded images"""

def create_degraded_images(image_files, local_input_dir, local_degraded_dir, drive_degraded_dir, scale_factor=0.5):
    """Create degraded versions of images with careful preservation of text elements"""
    degraded_files = []
    for filename in image_files:
        input_path = os.path.join(local_input_dir, filename)
        degraded_filename = os.path.splitext(filename)[0] + '_degraded' + os.path.splitext(filename)[1]
        degraded_path = os.path.join(local_degraded_dir, degraded_filename)

        img = cv2.imread(input_path, cv2.IMREAD_COLOR)
        if img is None:
            continue

        h, w = img.shape[:2]
        lr_h, lr_w = int(h * scale_factor), int(w * scale_factor)

        # Downscale with less aggressive degradation to preserve text readability
        img_lr = cv2.resize(img, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)

        # Less aggressive JPEG compression for text images
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 60]  # Higher quality for text
        _, buffer = cv2.imencode('.jpg', img_lr, encode_param)
        img_lr = cv2.imdecode(buffer, cv2.IMREAD_COLOR)

        # Very mild blur to preserve text edges
        img_lr = cv2.GaussianBlur(img_lr, (3, 3), 0.6)

        cv2.imwrite(degraded_path, img_lr)
        os.system(f'cp "{degraded_path}" "{os.path.join(drive_degraded_dir, degraded_filename)}"')
        degraded_files.append(degraded_filename)
    return degraded_files

"""## Initialize CLIP Model for Evaluation"""

def load_clip_model():
    """Load CLIP model"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

"""## OCR accuracy

"""

def calculate_ocr_accuracy(original_img_path, enhanced_img_path):
    """Calculate OCR accuracy for text in images with improved algorithm"""
    try:
        # Try to improve OCR text detection with image preprocessing
        def preprocess_image_for_ocr(img_path):
            img = cv2.imread(img_path)
            if img is None:
                return None

            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Apply thresholding to better separate text from background
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            # Use morphological operations to enhance text
            kernel = np.ones((1, 1), np.uint8)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)

            return thresh

        # Extract text using improved preprocessing
        orig_preprocessed = preprocess_image_for_ocr(original_img_path)
        enhanced_preprocessed = preprocess_image_for_ocr(enhanced_img_path)

        if orig_preprocessed is None or enhanced_preprocessed is None:
            print(f"Error: Could not read one of the images")
            return 0.0

        # Try multiple OCR configurations for better text detection
        config = r'--oem 3 --psm 6'  # Assume a single uniform block of text
        original_text = pytesseract.image_to_string(orig_preprocessed, config=config)
        enhanced_text = pytesseract.image_to_string(enhanced_preprocessed, config=config)

        # If no text detected, try different PSM mode
        if not original_text.strip():
            config = r'--oem 3 --psm 11'  # Sparse text, find as much text as possible
            original_text = pytesseract.image_to_string(orig_preprocessed, config=config)
            enhanced_text = pytesseract.image_to_string(enhanced_preprocessed, config=config)

        # If still no text, try direct detection with different settings
        if not original_text.strip():
            config = r'--oem 3 --psm 12'  # Dense text with OSD
            original_text = pytesseract.image_to_string(original_img_path, config=config)
            enhanced_text = pytesseract.image_to_string(enhanced_img_path, config=config)

        if not original_text.strip():
            print(f"Warning: No text detected in images, likely product labels only")
            return 0.0

        # Use a better text similarity metric - word-based Jaccard similarity
        def get_words(text):
            return set(text.lower().strip().split())

        original_words = get_words(original_text)
        enhanced_words = get_words(enhanced_text)

        if len(original_words) == 0:
            return 0.0

        # Use Jaccard similarity for a better comparison
        common_words = original_words.intersection(enhanced_words)
        all_words = original_words.union(enhanced_words)

        # Print detected words for debugging
        print(f"Original text words: {original_words}")
        print(f"Enhanced text words: {enhanced_words}")
        print(f"Common words: {common_words}")

        if len(all_words) == 0:
            return 0.0

        accuracy = len(common_words) / len(all_words) * 100
        return accuracy

    except Exception as e:
        print(f"Error calculating OCR accuracy: {e}")
        return 0.0

"""## Degradation Estimation function"""

def estimate_degradation(img):
    """Estimate degradation parameters of the input image"""
    # Convert to grayscale for analysis
    if len(img.shape) > 2:
        if img.shape[2] == 3:  # RGB
            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        else:  # Assuming BGR
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img.copy()

    # Estimate blur (using variance of Laplacian)
    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()

    # Estimate noise (using filter-based method)
    noise_score = estimate_noise(gray)

    # Estimate compression artifacts (using blockiness)
    compression_score = estimate_compression(gray)

    return {
        'blur': blur_score,
        'noise': noise_score,
        'compression': compression_score
    }

def estimate_noise(gray_img):
    """Estimate noise level in an image using high-frequency components"""
    blurred = cv2.GaussianBlur(gray_img, (5, 5), 0)
    noise = gray_img.astype(np.float32) - blurred.astype(np.float32)
    return np.std(noise)

def estimate_compression(gray_img):
    """Estimate JPEG compression level based on blockiness"""
    h, w = gray_img.shape
    block_size = 8  # JPEG uses 8x8 blocks

    # Check if image is too small
    if h < block_size * 2 or w < block_size * 2:
        return 0

    # Calculate blockiness score by measuring differences at block boundaries
    blockiness = 0
    for i in range(block_size, h, block_size):
        blockiness += np.sum(np.abs(gray_img[i,:].astype(float) - gray_img[i-1,:].astype(float)))

    for j in range(block_size, w, block_size):
        blockiness += np.sum(np.abs(gray_img[:,j].astype(float) - gray_img[:,j-1].astype(float)))

    # Normalize by number of pixels
    total_boundary_pixels = (h // block_size) * w + (w // block_size) * h
    if total_boundary_pixels > 0:
        blockiness /= total_boundary_pixels

    return blockiness

"""## Metrics Functions"""

def calculate_metrics(original_img, enhanced_img, clip_model=None, clip_preprocess=None, prompt=None, device=None):
    """Calculate image quality metrics"""
    if original_img.shape != enhanced_img.shape:
        enhanced_img = cv2.resize(enhanced_img, (original_img.shape[1], original_img.shape[0]),
                                 interpolation=cv2.INTER_CUBIC)

    def rgb_to_y(img):
        return 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]

    original_y = rgb_to_y(original_img)
    enhanced_y = rgb_to_y(enhanced_img)

    psnr = peak_signal_noise_ratio(original_y, enhanced_y, data_range=255)
    ssim = structural_similarity(original_y, enhanced_y, data_range=255)

    def calculate_sharpness(img):
        gray = img if len(img.shape) == 2 else rgb_to_y(img)
        return cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F).var()

    original_sharpness = calculate_sharpness(original_img)
    enhanced_sharpness = calculate_sharpness(enhanced_img)
    sharpness_increase = (enhanced_sharpness - original_sharpness) / (original_sharpness + 1e-8) * 100

    def calculate_details(img):
        gray = img if len(img.shape) == 2 else rgb_to_y(img)
        blurred = gaussian_filter(gray, sigma=3)
        high_freq = np.abs(gray - blurred)
        return np.mean(high_freq)

    original_details = calculate_details(original_img)
    enhanced_details = calculate_details(enhanced_img)
    detail_increase = (enhanced_details - original_details) / (original_details + 1e-8) * 100

    metrics = {
        'psnr': psnr,
        'ssim': ssim,
        'sharpness_increase': sharpness_increase,
        'detail_increase': detail_increase
    }

    # Add CLIP metrics if available
    if clip_model is not None and clip_preprocess is not None and device is not None:
        try:
            original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
            enhanced_rgb = cv2.cvtColor(enhanced_img, cv2.COLOR_BGR2RGB)

            original_pil = Image.fromarray(original_rgb)
            enhanced_pil = Image.fromarray(enhanced_rgb)

            with torch.no_grad():
                original_features = clip_model.encode_image(clip_preprocess(original_pil).unsqueeze(0).to(device))
                enhanced_features = clip_model.encode_image(clip_preprocess(enhanced_pil).unsqueeze(0).to(device))

                original_features = original_features / original_features.norm(dim=-1, keepdim=True)
                enhanced_features = enhanced_features / enhanced_features.norm(dim=-1, keepdim=True)

                clip_img_similarity = torch.nn.functional.cosine_similarity(original_features, enhanced_features).item()
                metrics['clip_img_similarity'] = clip_img_similarity

                if prompt:
                    text = clip.tokenize([prompt]).to(device)
                    text_features = clip_model.encode_text(text)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    prompt_img_similarity = torch.nn.functional.cosine_similarity(enhanced_features, text_features).item()
                    metrics['prompt_img_similarity'] = prompt_img_similarity
        except Exception as e:
            print(f"CLIP metric error: {e}")

    return metrics

"""## Define StableSR Model"""

def load_sdxl_model():
    """Load SDXL upscaler model with optimizations for text-heavy images"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    try:
        # Use the specialized upscaler model
        model_id = "stabilityai/stable-diffusion-x4-upscaler"

        # Load with optimized configuration
        pipe = StableDiffusionUpscalePipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            use_safetensors=True,
            variant="fp16" if device == "cuda" else None
        )

        # Use DDIM scheduler for better quality
        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)

        # Enable memory optimizations
        if device == "cuda":
            pipe.enable_model_cpu_offload()
            pipe.enable_vae_slicing()
            if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
                pipe.enable_xformers_memory_efficient_attention()

        print("Successfully loaded SDXL Upscaler model")
        return pipe.to(device), device
    except Exception as e:
        print(f"Error loading model: {e}")
        import traceback
        traceback.print_exc()
        return None, device

def get_text_prompts():
    """Get specific prompts optimized for text enhancement"""
    return [
        "Three vibrant soda cans (“Poppi” strawberry lemonade, orange, lemon-lime) with fresh water droplets on a bright pink-orange gradient background, photorealistic packaging render, ultra-detailed, super-resolution, crisp reflections and highlights on metal",
        "A sleek red Coca-Cola Zero Sugar can soaring through a clear blue sky, leaving a swirling cloud-trail spelling “Coca-Cola”, photorealistic product shot, ultra-detailed, super-resolution, soft cloud textures and metallic can highlights",
        "Retro poster of an ecstatic cartoon man holding a foaming beer mug, bold typography “BrewCo Draught House” and “Extreme beers are extremely satisfying”, stylized vintage halftone illustration, ultra-detailed, super-resolution, textured paper grain",
        "Classic poster of a torn-paper reveal showing a Campari bottle on a black background, elegant script “Bitter Campari l’aperitivo”, retro print style, ultra-detailed, super-resolution, subtle paper texture and distressed edges",
        "Vintage French ad poster: overhead view of a waiter’s tray with a Berger 45 bottle and two glasses on a bright yellow field, bold black “Berger 45 Type Marseillais” text, stylized flat colors, ultra-detailed, super-resolution, textured poster look",
        "Clean 300 ml glass juice bottle labeled “Tasty Drink” on a white background, fresh orange slice and whole orange behind, Healthy & Tasty logo and Arabic slogan, photorealistic packaging render, ultra-detailed, super-resolution, crisp glass reflections",
        "Sunset coastal road scene with red convertible parked at edge, warm orange sky and setting sun over the ocean, bold retro text “Experience the Open Road” and “Freedom in Every Drive”, stylized illustration, ultra-detailed, 8k super-resolution, film-grain texture",
        "Vintage-style ad of a glossy green muscle car on textured sand-colored paper, bold heading “A New Kind of Excitement!”, minimalist layout, ultra-detailed, super-resolution, subtle halftone print effect",
        "Retro automotive poster: red convertible Speedster 1972 on flat ochre background, bold black “Freedom on Wheels” and “0–60 in 7 seconds” text, stylized illustration, ultra-detailed, super-resolution, textured paper finish",
        "Classic car ad on aged beige paper: blue Falcon GT sedan with soft shadow, bold headline “Meet the Legend: Falcon GT – Power and Style in One”, retro print style, ultra-detailed, super-resolution, vintage grain texture"
    ]

"""## Process Images"""

def process_image_sdxl(degraded_path, output_path, pipe, device, prompt,
                       clip_model=None, clip_preprocess=None, original_path=None,
                       num_inference_steps=30):
    """Process a single image with SDXL upscaler optimized for text"""
    # Read degraded image
    img = cv2.imread(degraded_path, cv2.IMREAD_COLOR)
    if img is None:
        print(f"Error reading {degraded_path}")
        return None, None, None

    # Convert to RGB for PIL
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Estimate degradation parameters
    degradation = estimate_degradation(img_rgb)
    print(f"Degradation estimates: Blur={degradation['blur']:.2f}, Noise={degradation['noise']:.2f}, Compression={degradation['compression']:.2f}")

    # Calculate adaptive noise level but biased lower for text preservation
    # Lower noise for text images to preserve text clarity
    noise_level = int(min(50, max(10,
                           20 + degradation['noise'] * 1.5 - degradation['blur'] * 0.1 + degradation['compression'] * 3.0)))
    print(f"Using adaptive noise level: {noise_level}")

    # Convert to PIL Image for the pipeline
    img_pil = Image.fromarray(img_rgb)

    # Start timer
    start_time = time.time()

    try:
        # Clear GPU memory before processing
        clear_gpu_memory()

        # Process with the upscaler pipeline - use lower guidance scale for text preservation
        with torch.no_grad():  # Reduce memory usage
            result = pipe(
                prompt=prompt,
                image=img_pil,
                noise_level=noise_level,
                num_inference_steps=num_inference_steps,
                guidance_scale=6.5,  # Slightly lower for text
            ).images[0]

        # Convert back to OpenCV format
        enhanced_img = cv2.cvtColor(np.array(result), cv2.COLOR_RGB2BGR)

        # Calculate processing time
        process_time = time.time() - start_time
        print(f"Processing time: {process_time:.2f} seconds")

        # Apply text-enhancing sharpening post-processing
        kernel = np.array([[-0.5, -0.5, -0.5], [-0.5, 5, -0.5], [-0.5, -0.5, -0.5]])
        enhanced_img = cv2.filter2D(enhanced_img, -1, kernel)

        # Save the enhanced output
        cv2.imwrite(output_path, enhanced_img)
        print(f"Saved enhanced image to {output_path}")

        # Calculate metrics
        metrics = calculate_metrics(
            img, enhanced_img,
            clip_model=clip_model,
            clip_preprocess=clip_preprocess,
            prompt=prompt,
            device=device
        )

        # Calculate OCR accuracy if original path is provided
        if original_path:
            ocr_accuracy = calculate_ocr_accuracy(original_path, output_path)
            metrics['ocr_accuracy'] = ocr_accuracy
            print(f"OCR Accuracy: {ocr_accuracy:.2f}%")

        # Print metrics
        print("\nImage Quality Metrics:")
        print(f"PSNR: {metrics['psnr']:.2f} dB")
        print(f"SSIM: {metrics['ssim']:.4f}")
        print(f"Sharpness increase: {metrics['sharpness_increase']:.2f}%")
        print(f"Detail increase: {metrics['detail_increase']:.2f}%")
        if 'ocr_accuracy' in metrics:
            print(f"OCR Accuracy: {metrics['ocr_accuracy']:.2f}%")
        if 'clip_img_similarity' in metrics:
            print(f"CLIP Image Similarity: {metrics['clip_img_similarity']:.4f}")
        if 'prompt_img_similarity' in metrics:
            print(f"Prompt-Image Similarity: {metrics['prompt_img_similarity']:.4f}")

        return img, enhanced_img, metrics

    except Exception as e:
        print(f"Error processing image: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def process_all_images_sdxl(image_files, pipe, device, clip_model, clip_preprocess,
                           local_input_dir, local_degraded_dir, local_output_dir, drive_output_dir,
                           prompts=None):
    """Process all images with SDXL upscaler"""
    all_metrics = []

    # Make sure we have enough prompts (repeat if necessary)
    if prompts and len(prompts) < len(image_files):
        prompts = (prompts * ((len(image_files) // len(prompts)) + 1))[:len(image_files)]

    for i, filename in enumerate(image_files):
        # Clear memory before each image
        clear_gpu_memory()

        base_filename = os.path.splitext(filename)[0]
        degraded_filename = base_filename + '_degraded' + os.path.splitext(filename)[1]

        input_path = os.path.join(local_input_dir, filename)
        degraded_path = os.path.join(local_degraded_dir, degraded_filename)
        output_filename = base_filename + '_SDXL.png'
        output_path = os.path.join(local_output_dir, output_filename)

        # Get prompt for this image
        prompt = prompts[i] if prompts and i < len(prompts) else "high quality product photograph with clear readable text"

        print(f"\n{'='*80}\nProcessing {filename} with SDXL Upscaler...")
        print(f"Prompt: {prompt}")

        try:
            degraded_img, output_img, metrics = process_image_sdxl(
                degraded_path, output_path, pipe, device, prompt,
                clip_model=clip_model, clip_preprocess=clip_preprocess, original_path=input_path
            )

            if degraded_img is None or output_img is None:
                print(f"Error processing {filename}")
                continue

            # Store metrics for later analysis
            metrics['filename'] = filename
            metrics['model'] = "SDXL"
            all_metrics.append(metrics)

            # Copy to Google Drive
            drive_output_path = os.path.join(drive_output_dir, output_filename)
            os.system(f'cp "{output_path}" "{drive_output_path}"')
            print(f"Saved to Google Drive: {drive_output_path}")

            # Read original image for display
            orig_img = cv2.imread(input_path, cv2.IMREAD_COLOR)
            orig_img_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
            degraded_img_rgb = cv2.cvtColor(degraded_img, cv2.COLOR_BGR2RGB)
            output_img_rgb = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)

            # Display comparison
            plt.figure(figsize=(20, 12))

            # Original image
            plt.subplot(1, 3, 1)
            plt.imshow(orig_img_rgb)
            plt.title(f"Original: {filename}\nDimensions: {orig_img.shape[1]}x{orig_img.shape[0]}", fontsize=12)
            plt.axis('off')

            # Degraded image
            plt.subplot(1, 3, 2)
            degraded_resized = cv2.resize(degraded_img_rgb, (orig_img.shape[1], orig_img.shape[0]), interpolation=cv2.INTER_NEAREST)
            plt.imshow(degraded_resized)
            plt.title(f"Degraded: {degraded_filename}\nDimensions: {degraded_img.shape[1]}x{degraded_img.shape[0]}\n(Resized for display)", fontsize=12)
            plt.axis('off')

            # Enhanced image
            plt.subplot(1, 3, 3)
            enhanced_resized = cv2.resize(output_img_rgb, (orig_img.shape[1], orig_img.shape[0]), interpolation=cv2.INTER_CUBIC) if output_img.shape != orig_img.shape else output_img_rgb
            plt.imshow(enhanced_resized)

            title_text = f"Enhanced (SDXL): {output_filename}\nDimensions: {output_img.shape[1]}x{output_img.shape[0]}\n"
            title_text += f"PSNR: {metrics['psnr']:.2f} dB, SSIM: {metrics['ssim']:.4f}\n"
            title_text += f"Sharpness Inc: {metrics['sharpness_increase']:.2f}%, Detail Inc: {metrics['detail_increase']:.2f}%"

            if 'ocr_accuracy' in metrics:
                title_text += f"\nOCR Accuracy: {metrics['ocr_accuracy']:.2f}%"
            if 'clip_img_similarity' in metrics:
                title_text += f"\nCLIP Img Sim: {metrics['clip_img_similarity']:.4f}"
            if 'prompt_img_similarity' in metrics:
                title_text += f", Prompt Sim: {metrics['prompt_img_similarity']:.4f}"

            plt.title(title_text, fontsize=12)
            plt.axis('off')

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Error processing {filename}: {e}")
            import traceback
            traceback.print_exc()

    print(f"Processing with SDXL Upscaler complete!")
    return all_metrics

"""

## Analyzing Results Function"""

def analyze_results(all_results, model_name="SDXL"):
    """Analyze and visualize results including OCR metrics"""
    if not all_results:
        print("No results to analyze")
        return

    metrics_to_average = ['psnr', 'ssim', 'sharpness_increase', 'detail_increase',
                        'clip_img_similarity', 'prompt_img_similarity', 'ocr_accuracy']
    avg_metrics = {}

    for metric in metrics_to_average:
        valid_results = [r[metric] for r in all_results if metric in r]
        if valid_results:
            avg_metrics[metric] = sum(valid_results) / len(valid_results)

    print(f"\nAverage Metrics for {model_name}:")
    print(f"{'Metric':<20} {'Value':<15}")
    print("-" * 35)
    for metric, value in avg_metrics.items():
        print(f"{metric:<20} {value:<15.4f}")

    # Create visualization
    metrics_to_plot = ['psnr', 'ssim', 'clip_img_similarity', 'ocr_accuracy']
    labels = ['PSNR (dB)', 'SSIM (x100)', 'CLIP Image Similarity (x100)', 'OCR Accuracy (%)']

    values = []
    for i, metric in enumerate(metrics_to_plot):
        if metric in avg_metrics:
            # Scale SSIM and CLIP similarity for better visualization
            if metric == 'ssim' or metric == 'clip_img_similarity':
                values.append(avg_metrics[metric] * 100)
            else:
                values.append(avg_metrics[metric])
        else:
            values.append(0)

    plt.figure(figsize=(12, 6))
    bars = plt.bar(labels, values, color=['skyblue', 'lightgreen', 'coral', 'gold'])

    for bar, value in zip(bars, values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{value:.2f}', ha='center', va='bottom')

    plt.ylabel('Values')
    plt.title(f'Average Image Quality Metrics - {model_name}')
    plt.ylim(0, max(values) * 1.2)

    plt.tight_layout()
    plt.show()

    # Detailed analysis of each metric
    for metric in metrics_to_average:
        if all(metric in result for result in all_results):
            values = [result[metric] for result in all_results]
            filenames = [result['filename'] for result in all_results]

            plt.figure(figsize=(14, 6))
            bars = plt.bar(filenames, values, color='skyblue')

            plt.title(f'{metric.replace("_", " ").title()} by Image')
            plt.xticks(rotation=45, ha='right')
            plt.ylabel(metric.replace("_", " ").title())
            plt.tight_layout()
            plt.show()

"""## Main Part"""

def main():
    # Copy images from Drive
    image_files = copy_images_from_drive(drive_input_dir, local_input_dir)

    if not image_files:
        print("No images found!")
        return

    # Create degraded versions
    print("\n=== Creating degraded versions of images ===")
    create_degraded_images(image_files, local_input_dir, local_degraded_dir, drive_degraded_dir, scale_factor=0.5)

    # Load CLIP model for evaluation
    print("\n=== Loading CLIP model for evaluation ===")
    clip_model, clip_preprocess, clip_device = load_clip_model()

    # Load SDXL model
    print("\n=== Loading SDXL Upscaler model ===")
    pipe, device = load_sdxl_model()

    if pipe is None:
        print("Failed to load model. Exiting.")
        return

    # Get prompts for text images
    prompts = get_text_prompts()

    # Process images with SDXL Upscaler
    print("\n=== Processing with SDXL Upscaler ===")
    results = process_all_images_sdxl(
        image_files=image_files,
        pipe=pipe,
        device=device,
        clip_model=clip_model,
        clip_preprocess=clip_preprocess,
        local_input_dir=local_input_dir,
        local_degraded_dir=local_degraded_dir,
        local_output_dir=local_output_dir,
        drive_output_dir=drive_output_dir,
        prompts=prompts
    )

    # Analyze results
    print("\n=== Analyzing Results ===")
    analyze_results(results, "SDXL")

    print(f"\nProcessing complete. Results saved to: {drive_output_dir}")

if __name__ == "__main__":
    main()

